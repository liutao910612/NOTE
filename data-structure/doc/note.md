# 1概述
**数据结构和算法的定义**   
从广义上讲，数据结构就是指一组数据的存储结构。算法就是指操作数据的一组方法。   
从狭义上讲，是指某些著名的数据结构和算法，比如队列、栈、堆、二分查找和动态规划等。  
**数据结构和算法的关系**   
数据结构是为算法服务的，算法要作用在特定的数据结构上。  
**算法知识点总览**   
![avatar](pg/summary.jpg)   
# 2如何分析统计算法的执行效率和资源消耗
## 大O复杂度表示法
所有代码的执行时间与每行代码的执行时间成正比 ：**T(n)=O(f(n))**   
+ T(n)表示代码的执行时间
+ n表示数据规模的大小   
+ f(n)表示每行代码执行的次数总和，因为是一个公式，所以用f(n)表示   
+ O表示代码的执行时间T(n)与执行次数f(n)成正比   
大O时间复杂度实际上并不具体表示代码的真正执行时间，而是表示**代码执行时间随着数据规模增长的变化趋势**，所以也叫做**渐进时间复杂度**，简称**时间复杂度。**   
## 时间复杂度
时间复杂度就叫渐进时间复杂度，表示算法的执行时间与数据规模之间的关系。
## 如何分析一段代码的时间复杂度
1. 只关注循环次数最多的一段代码   
2. 加法法则：总复杂度等于量级最大的那段代码的复杂度   
这里需要强调时间复杂度表示的是算法执行效率与数据规模增长的变化趋势，而不是代码的执行时间。   
时间复杂度的加法法则公式：如果T1(n)=O(f(n)),T2(n)=O(g(n));那么T(n)=T1(n)+T2(n)=max(O(f(n)),O(g(n)))=O(max(f(n),g(n)))   
3. 乘法法则：嵌套代码复杂度等于嵌套内外代码复杂度的乘积   
时间复杂度的乘法法则公式：如果T1(n)=O(f(n)),T2(n)=O(g(n));那么T(n)=T1(n)*T2(n)=O(f(n))*O(g(n))=O(f(n)*g(n))   
## 几种常见时间复杂度实例分析
常见时间复杂度量级如下：    
+ 常量阶O(1)
+ 对数阶O(logn)
+ 线性阶O(n)
+ 线性对数阶O(nlogn)
+ 指数阶O(2^n) - 非多项式量级
+ 阶乘阶 O(n!) - 非多项式量级
+ 平方阶O(n^2)、立方阶O(n^3)...k次方阶O(n^k)   
NP(非确定多项式)问题：时间复杂度为**非多项式量级**的算法问题。   
随着数据规模的增大，非多项式量级算法的执行时间会急速增长。因此主要讨论**多项式量级时间复杂度**。
1. O(1)
只要代码的执行时间不会随着n(数据量)的增大而增大，这样代码的执行时间复杂度都记作O(1)。   
一般情况下，只要算法中不存在循环语句、递归语句，即使有成千上万行的代码，其时间复杂度也是O(1)。   
2. O(logn)、O(nlogn)
对数阶的示例代码如下：   
```java
i=1;
while (i <= n) {
    i = i * 2; 
}
``` 
线性对数阶O(nlogn)，根据时间复杂度的乘法法则,O(nlogn) = O(n)*O(logn),比如如下代码：   
```java

i=1;
while (i <= n) {
    i = i * 2; 
    test(n);
}

void test(n){
    int summary = 0;
    for(i = 1; i < n ;i++){
        summary += i;
    }
}
``` 
在执行时间复杂度为O(logn)的代码的同时调用了时间复杂度为O(n)的方法。   
3. O(m+n)、O(m*n)   
当代码的时间复杂度由两个数据的规模来决定的时候。   
```java
int cal(int m,int n){
    int sum1 = 0;
    for(i=1;i<m;i++){
        sum1 += i;
    }
    
    int sum2 = 0;
    for(i=1;i<n;i++){
        sum2 += i;
    }
    return sum1+sum2;
}
``` 
上面的代码和前面提到的加法法则的区别在于：这里的代码没法确定哪个复杂度的量级更大。所以T1(m)+T2(n)=O(f(m)+g(n))。但是乘法法则依然有效。   
## 空间复杂度分析
空间复杂度又叫渐进空间复杂度表示算法的存储空间与数据规模之间的关系。   
空间复杂度的分析相对于时间复杂度比较简单。   
```java
void print(int n){
    int i = 0;  //1
    int[] a = new int[n]; //2
    for(i;i<n;i++){
        a[i]=i*i;
    }
    
    for(i;i<n;i++){
        print out a[i];
    }
}
``` 
上面这段代码，1处申请了一个空间存储变量，2处申请了n个空间存储变量，其余代码几乎没有占用存储空间，所以这段代码的空间复杂度为O(n)。
常见的空间复杂度为O(1)，O(n)，O(n^2)，对数阶的空间复杂度很少用到。   
越高阶的复杂度算法执行效率越低。   
**复杂度从低阶到高阶** : O(1) < O(logn) < O(n) < O(nlogn)< O(n^2)   
# 3最好、最坏、平均和均摊时间复杂度
```java
// n表示数组array的长度
int find(int[] array, int n, int x) {
    int i = 0;
    int pos = -1;
    for (; i < n; ++i) {
        if (array[i] == x) { 
            pos = i; break; 
        } 
    }
    return pos;
}
```    
## 3.1最好、最坏情况时间复杂度
**最好情况时间复杂度**：在最理想的情况下执行这段代码的时间复杂度。上面这段代码的最好情况时间复杂度为O(1)。
**最坏情况时间复杂度**：在最糟糕的情况下执行这段代码的时间复杂度。上面这段代码的最好情况时间复杂度为O(n)。
## 3.2平均情况时间复杂度
上面的代码我们假设查找的数据在数组中和不在数组中的概率分别为1/2。在数组中的情况，出现在每个位置的概率为1/n。那么平均时间复杂度
的计算就为：1*1/2n + 2*1/2n + 3*1/2n + ... + n *1/2n + n*1/n = (3n+1)/4。    
这个值就是概率论中的加权平均值，也叫做期望值，平均时间复杂度的全称应该叫做：**加权平均时间复杂度** 或者 **期望时间复杂度** 。   
平均时间复杂度为：O(n)   
通常情况下我们不需要对时间复杂度进行这么详细的分析，仅仅当在不同的情况下，时间复杂度有量级的差距的时候，我们才会使用这三种复杂度来表示。   
## 3.3 均摊时间复杂度
均摊时间复杂度对应的分析方法为摊还分析（或者叫平摊分析）。   
对一个数据结构进行一组连续操作中，大部分情况下时间复杂度都很低，只有个别情况下时间复杂度比较高，而且这些操作之间有前后连续的时序关系，
这个时候我们就将这一组操作放在一块儿分析，看能否将较高的时间复杂度那次操作的耗时，平摊到那些时间复杂度比较低的操作上。而且
，在能够运用均摊时间复杂度分析的场合，一般均摊时间复杂度就等于最好情况时间复杂度。
# 4解密数组从0开始编号
什么是数组 ： 数组是一种线性表数据结构，用一组连续的内存空间，来存储一组具有相同类型的数据结构。   
线性表：数据排列成像线一样的结构。每个线性表上的数据最多只有前后两个方向。除了数组，链表、队列和栈等都是线性表结构。   
与线性表对应的是**非线性表**，比如二叉树、堆和图等。之所以叫做非线性表，是因为，在非线性表的数据结构中，数据之间并不是简单的前后关系。   
数组根据下标随机访问的时间复杂度为O(1)。   
从数组存储的内存模型上来看，“下标”最确切的定义应该是“偏移”。如果用a来表示数组的首地址，a[0]就是偏移为0的位置，也就是首地址，a[k]就
表示偏移k个type_size位置，所以计算a[k]的内存地址只需要用这个公式：    
a[k]_address=base_address+k*type_size    
如果从1开始计算，那么我们计算内存地址就变成了：    
a[k]_address=base_address+（k-1）*type_size    
从上面我们可以看出，如果从1开始编号，每次进行随机下标访问就会多一次减法操作，对CPU来说，就会多一次减法指令。   
数组作为非常基础的数据结构，通过下标随机访问数组元素又是其非常基础的编程操作，效率的优化就要尽可能做到极致。
所以为了减少一次减法操作，数组选择了从0开始编号，而不是从1开始。   
# 5链表（上）：如何实现LRU缓存淘汰算法
常见的缓存策略：先进先出策略（FIFO）、最少使用策略（LFU）和最近最少使用策略（LRU）。   
链表：单链表、双链表和循环链表   
使用单链表来实现LRU缓存。当我们访问一个数据的时候，如果此数据存在链表中，我们就直接将原本位置的数据取出来，并且放置
到链表的首位置，这样就能保证访问次数最多的数据放到了链表的最开始位置。如果没有被访问的数据就放到了链表的尾部。
# 6链表（下）：如何轻松写出正确的链表代码
指针或者引用的含义：指针和引用都是存储所指对象的内存地址。   
将某个变量赋值给引用，实际上就是将变量的内存地址赋值给引用，或者反过来说引用中存储了这个变量的内存地址，指向了这个变量，通过引用
就能找到这个变量。   
哨兵：如果我们引入哨兵节点，在任何时候，不管链表是不是空的，head都会一直指向这个哨兵节点。我们也把这种有哨兵节点的链表叫做
带头链表。   
# 7栈：如何实现浏览器的前进和后退功能
栈：后进者先出，先进者后出。   
当某个数据集只涉及在一端插入或者删除数据，并且满足后进者先出，先进者后出的特性，这个时候我们就应该首选“栈”这种数据结构。   
栈的实现：既可以使用数组来实现栈，也可以使用链表来实现栈，用数组实现的栈叫做顺序栈，用链表实现的栈叫做链式栈。   
# 8队列：队列在线程池等有限资源中的应用
队列：先进者先出，这就是典型的“队列”。   
队列跟栈一样，也是一种操作受限的线性表数据结构。   
跟栈一样，队列可以用数组来实现，也可以用链表来实现。用数组实现的队列叫做顺序队列，用链表实现的队列叫做链式队列。   
对于大部分资源有限的场景，当没有空闲资源的时候，都可以使用队列这种数据结构来实现排队等待。   
# 9递归：如何用三行代码找到“最终推荐人”
递归需要满足的三个条件：   
1. 一个问题的解可以分解为几个子问题的解。
2. 这个问题与分解后的子问题，除了数据规模不同，求解思路完全一样。
3. 存在递归终止条件。   
写递归代码的关键是找到将大问题分解成小问题的规律，基于此写出递推公式，并且找到种终止条件。最后将递推公式
和终止条件翻译成代码。   
# 10排序（上）：为什么插入排序比冒泡排序更受欢迎？
**如何分析一个排序算法？**   
(1)排序算法的执行效率。     
a.最好情况，最坏情况和平均情况时间复杂度。   
b.时间复杂度的系数、常数和低阶。   
c.比较次数和交换次数。   
(2)排序算法的内存消耗。   
原地排序：空间复杂度为O(1)的排序。   
(3)排序算法的稳定性。   
稳定性是指如果待排序的序列中存在相等的元素，排序前后相等元素的顺序不变。  
# 11排序下：如何用快速排序思想在O(n)内查找第K大的元素？
归并排序和快速排序都是采用分治的思想，快速排序是按照某个元素来进行分区的，即大于某个元素放入一个区域，小于
某个元素放入另一个区域。而归并排序是利用数组下标进行分区的，下标大于分区下标在一个区域，小于分区下标在另一
个区域。

# 12线性排序
线性排序共有3中，分别为桶排序、计数排序和   
1.桶排序   
核心思想 :将需要排序的数据分别放到几个桶里，在分别将每个桶里的数据进行排序，排序完成后再将每个桶里的数据按照
顺序依次取出，组成的序列就是有序的了。   
先决条件：要排序的数据很容易就能划分成m个桶，并且桶与桶之间有着天然的大小顺序。这样每个桶内的数据排序完后，桶
与桶之间的数据就不需要再进行排序了。其次数据再各个桶之间的分布是比较均匀的。   
适用场景：比较适合用在外部排序中。所谓外部排序就是数据存储再外部磁盘中，数据量比较大，内存有限，无法将数据全部
加载到内存中。  
复杂度为O(n)
2.计数排序   
计数排序其实是桶排序的一种特殊情况，当要排序n个数据，所处的范围并不大的时候，比如最大值是k，我们就可以把数据划
分成k个桶。   
复杂度为O(n)。   
3.基数排序   
# 13排序优化
# 14二分查找(上)
二分查找针对的是一个有序的数据集合，查找思想有点类似分治思想。每次都通过跟区间的中间元素对比，将待查找的区间缩小
一半，直到找到查找的元素，或者区间被缩小为0。   
二分查找的时间复杂度分析：   
我们将设数据量的大小为n，每次查找后数据都会缩小为原来的一半，也就是会除以2，最坏的情况下直到查找的区间被缩小为空，
才停止。  
这样一来，查找区间的变化为n,n/2，n/4,n/8,...,n/2^k,...。其中k是查找的次数。当查找停止的时候，也就是n/2^k=1
。这个时候求得k=log2n,所以复杂度就是O(logn)。   
常量级时间复杂度O(1)为什么有些时候可能没有指数时间复杂度O(logn)的算法执行效率高？   
# 15二分查找(下)
# 16跳表
链表加多级索引的结构 -> 跳表   
跳表的时间复杂度为O(logn)。   
# 17散列表（上）
散列表用的是数组支持按照下标随机访问数据的特性，所以散列表其实就是数组的一种扩展，由数组演化而来。可以说没有数组，就
没有散列表。    
规律：散列表用的就是数组支持按照下标随机访问的时候，时间复杂度为O(1)的特性。我们通过散列函数把元素的键值映射为数组下
标，然后将数据存储到对应的数组下标的位置。当我们按照键值查询元素的时候，我们用同样的散列函数将键值转换为数组下标,从对
应的下标位置取出元素。   
## 1.散列函数
散列函数，顾名思义，它是一个函数。我们可以把它定义成hash(key)，其中key表示元素的值，hash(key)的值表示经过散列函数
计算得到的散列值。   
散列函数设计的基本要求：   
1. 散列函数计算得到的值必须是一个非负整数。（因为数组的下标必须是整数，所以这里要求必须是非负整数）
2. 如果key1 = key2,那hash(key1) == hash(key2)。
3. 如果key1 != key2,那hash(key1) != hash(key2)。
针对上面的第三点就是所谓的hash冲突，现实中是不存在完全没有hash冲突的hash函数的，所以这里就需要我们解决hash冲突了。   
## 2.散列冲突
再好的散列函数也无法避免散列冲突。那究竟该如何解决散列冲突问题呢？我们常用的散列冲突解决方法有两类，开放寻址法（open
 addressing）和链表法（chaining）。    
1.开放寻址法   
开放寻址法的核心思想就是，如果出现了散列冲突，我们就重新探寻一个空闲位置，将其插入。
不管采用哪种探测方法，当散列表中空闲位置不多的时候，散列冲突就会大大的提高。为了尽可能保证散列表的操作效率，一般情况下
我们会尽可能保证散列表中有一定比例的空槽。我们用装载因子来表示空位的多少。   
装载因子越大，说明空闲位置越少，冲突越多，散列表的性能会下降。   
2.链表法   
# 18 散列表（中）：如何打造一个工业级别的散列表
**散列表碰撞攻击的基本原理：**通过精心构造的数据，使得数据被散列后都再同一个槽里面。这种情况下如果我们使用的是基于链表
的冲突解决方法，散列表就会退化为链表，查询的时间复杂度就会从O(1)变成O(n)。如果散列表中有10万个数据，退化后的散列表的查
询效率就会下降十万倍，也就是之前查询100条数据需要0.1秒，现在就需要1万秒。这样就有可能因为查询操作消耗大量 CPU 或者线
程资源，导致系统无法响应其他请求，从而达到拒绝服务攻击（DoS）的目的。   
**如何设计一个可以应对各种异常情况的工业级散列表，来避免在散列冲突的情况下，散列表性能的急剧下降，并且能抵抗散列碰
撞攻击？**   
**散列函数的设计：**   
函数的设计不能太复杂。   
散列函数生成的值要尽可能随机并且均匀分布。   
**如何选择冲突解决方法：**    
当数据量比较小、装载因子小的时候，适合采用开放寻址法。这也是 Java 中的ThreadLocalMap使用开放寻址法解决散
列冲突的原因。
基于链表的散列冲突处理方法比较适合存储大对象、大数据量的散列表，而且，比起开放寻址法，它更加灵活，
支持更多的优化策略，比如用红黑树代替链表。    
# 19 散列表（下）：为什么散列表和链表经常一起使用
# 20 哈希算法
## 1什么是哈希算法
将任意长度的二进制串映射成固定长度的二进制串，这个映射的规则就是**哈希算法**。   
通过原始的二进制串映射后得到的二进制串就是**哈希值**。    
优秀哈希算法设计的几点要求：    
1. 从哈希值不能反向推导出原始数据。（所以哈希算法也叫做单向哈希算法）
2. 对输入数据非常敏感，哪怕原始数据只修改了一个Bit,最后得到的哈希值也大不相同。
3. 哈希冲突的概率要很小，对于不同的原始数据，哈希值相同的概率非常小。    
4. 哈希算法的执行效率要高效，针对较长的文本也能快速计算出哈希值。   
## 2哈希算法的应用
**（一）安全加密**   
**（二）唯一标识**   
**（三）数据校验**   
**（四）散列函数**   
**（五）负载均衡**   
通过哈希算法对客户端IP地址或者会话ID计算哈希值，将取得的哈希值与服务器列表的大小进行取模运算，最终得到的只，就
是服务器的编号。这样我们就可以将相同IP过来的所有请求路由到同一个服务器上。   
**（六）数据分片**    
这里举例说明，比如说如何计算1T的日志文件（这里面记录了所有的搜索关键词），我们想要快速计算出每个关键词被搜索的次
数。这里的难点是日志文件很大，没法将整个日志文件放到计算机的内存中，另一方面即使能够放到计算机的内存中，计算过程
也会非常耗时。我们可以**将数据进行分片，然后利用多台机器来进行并行处理，从而提高处理速度**。具体思路是：我们
利用n台机器来处理，将数据读取出来，然后对数据进行哈希计算，将得到的哈希值对服务器的数量取模，最终将得到的服务器编
号，将数据放到对应的服务器编号就行了，这样相同的数据就能够被放置到同一台机器上，我们就可以将每台机器的计算结果加起
来。   
**（七）分布式存储**    
利用一致性哈希算法，可以解决缓存等分布式系统的扩容、缩容导致数据大量搬移的难题。    
# 21 二叉树
# 21.1 树、二叉树
**1.树**   
![avatar](pg/tree.jpg)    
上图中的每个元素叫做**节点**，用来连接相邻节点之间的关系叫做**父子关系**。   
没有父节点的节点叫做**根节点**，没有子节点的节点叫做**叶子节点或者叶节点**。   
节点的高度：节点到叶子节点的最长路径（边数）。   
节点的深度：根节点到这个节点所经历的边的个数。   
节点的层数：节点的深度+1。   
树的高度：根节点的高度。   
**2.二叉树**      
每个节点有两个叉，也就是两个子节点，分别是左子节点和右子节点。   
叶子节点都在最底层，除了叶子节点外，每个节点都有左右两个子节点，这种二叉树就叫做**满二叉树**。   
叶子节点都在最底下两层，最后一层的叶子节点都靠左排列，并且除了最后一层，其他层的节点个数都要达到最大，这种二叉树叫做
**完全二叉树**。   
存储二叉树的两种方法：一种是基于指针或引用的二叉链式存储法，一种是基于数组的顺序存储法。   
**2.二叉树的遍历**   
前序遍历是指，对于树中的任意节点来说，先打印这个节点，然后再打印它的左子树，最后打印它的右子树。   
中序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它本身，最后打印它的右子树。    
后序遍历是指，对于树中的任意节点来说，先打印它的左子树，然后再打印它的右子树，最后打印这个节点本身。  
二叉树遍历的时间复杂度为O(n)。
# 21.2 二叉树查找
二叉查找树的最大特点就是，支持动态数据集合的快速插入、删除和查找操作。   
二叉查找树的要求：在树中任意一个节点，其左子树的每个节点的值都要小于这个节点的值，其右子树的每个节点的值都要大于这个节
点的值。   
![avatar](pg/binarySearchTree.jpg)   
二叉查找树为何支持快速查找、删除和插入操作？   
**1.查找操作**   
当我们查找一个数的时候，我们先取根节点来和要查中的数据对比，如果相等就直接返回。如果小于根节点，就在左子树中遍历查找。如
果大于就在右子树中遍历查找。   
**2.插入操作**   
如果要插入的数据比节点的数据大，并且节点的右子树为空，就将新数据直接插入到右子节点的位置。如果不为空，就在遍历右子树，查
找插入的位置。同理，如果插入的数据比节点数值小，并且节点的左子树为空，就将新数据插入到左子节点的位置；如果不为空就再遍历
左子树，直到插入位置。   
**3.删除操作**    
删除操作相对与查找和插入操作来说要复杂一些，我们这里分为三种情况。   
1. 如果要删除的节点没有子节点，我们就直接将父节点中指向删除节点的指针置为null;
2. 如果要删除节点只有一个子节点，我们直接将父节点中指向子节点的指针指向要删除节点的子节点。
3. 如果要删除的节点有两个子节点，我们需要找到这个节点的右子树中的最小节点，把它替换到要删除的节点上。然后再删除掉这个最小
节点，因为最小节点肯定没有左子节点（如果有左子结点，那就不是最小节点了）。   
# 21.3 平衡二叉查找树、红黑树
**1.平衡二叉树**   
二叉树中任意一个节点的左右子树的高度差不能大于1。    
**平衡二叉查找树中“平衡”的意思，其实就是让整棵树左右看起来比较“对称”、比较“平衡”，不要出现左子树很高、右子树很矮的情
况。这样就能让整棵树的高度相对来说低一些，相应的插入、删除、查找等操作的效率高一些。**   
**2.红黑树**   
顾名思义，红黑树中的节点，一类被标记成黑色，一类被标记成红色。除此之外，一颗红黑树还需要满足这样几个要求：   
+ 根节点是黑色的；
+ 每个叶子节点都是黑色的空节点（NIL），也就是说叶子节点不存储数据；
+ 任何相邻的节点都不能同时为红色，也就是说，红色节点是被黑色节点隔开的；
+ 每个节点，从该节点到达叶子节点的所有路径，都包含相同数目的黑色节点；
# 21.4 递归树
递归的思想就是，将大问题分解成小问题来求解，然后再将小问题分解为小小问题。这样一层一层地分解，直到问题的数据规模被分解得足够小
，不用继续递归分解为止。如果我们把这个一层一层的分解过程画成图，它其实就是一棵树。我们给这棵树起一个名字，叫作递归树。    
![avatar](pg/tree2.jpg)   
我这里画了一棵斐波那契数列的递归树，你可以看看。节点里的数字表示数据的规模，一个节点的求解可以分解为左右子节点两个问题的求解。
# 22 堆和堆排序
堆必须满足下面两个条件：    
+ 堆是一个完全二叉树；
+ 堆中每个节点都必须大于等于（或小于等于）其子树中每个节点的值。如果每个节点都大于等于其子树中每个节点的值叫做大顶堆，如果小于
等于其子树中每个节点的值叫做小顶堆。   
**1. 往堆中插入一个元素**   
我们可以将插入元素放到堆的最后面，然后从下往上进行堆化。我们可以让新插入的节点与父节点对比大小。如果不满足子节点小于等于父节点的
大小关系，我们就互换两个节点。一直重复这个过程，直到父子节点之间满足刚说的那种大小关系。    
**2.删除对顶元素**      
我们把最后一个节点放到堆顶，然后利用同样的父子节点对比方法。对于不满足父子节点大小关系的，互换两个节点，并且重复进行这个过程，直
到父子节点之间满足大小关系为止。这就是从上往下的堆化方法。
# 23 图的表示
图中的元素叫做**顶点**，图中的一个顶点可以与任意顶点建立连接关系，我们把这种建立的关系叫做**边**,跟顶点相连的边的条数叫做顶点的
**度**。   
![avatar](pg/graph.jpg)    
我们将拥有方向的图叫做**有向图**，没有方向的图叫做**无向图**。与无向图中的度对应，有向图中有**入度**和**出度**。   
带权图（weighted graph）。在带权图中，每条边都有一个权重（weight）。   
**如何在内存中存储图这种数据结构：**   
1. 邻接矩阵存储法   
![avatar](pg/graph2.jpg) 
邻接矩阵的底层依赖一个二维数组。对于无向图来说，如果顶点 i 与顶点 j 之间有边，我们就将 A[i][j]和 A[j][i]标记为 1；对于有向图
来说，如果顶点 i 到顶点 j 之间，有一条箭头从顶点 i 指向顶点 j 的边，那我们就将 A[i][j]标记为 1。同理，如果有一条箭头从顶点 j
指向顶点 i 的边，我们就将 A[j][i]标记为 1。对于带权图，数组中就存储相应的权重。   
2. 邻接表存储方法   
![avatar](pg/adjacencyList.jpg)    
邻接表是不是有点像散列表？每个顶点对应一条链表，链表中存储的是与这个顶点相连接的其他顶点。另外我需要说明一下，图中画的是一个有向图
的邻接表存储方式，每个顶点对应的链表里面，存储的是指向的顶点。对于无向图来说，也是类似的，不过，每个顶点的链表中存储的，是跟这个顶
点有边相连的顶点。   
# 24 深度和广度优先搜索
算法是作用于具体的数据结构的，深度优先搜索算法和广度优先搜索算法都是基于“图”这种数据结构的。这是因为，图这种数据结构的表达能力很强，
大部分涉及搜索的场景都可以抽象成图。   
图上的搜索算法，最直接的理解就是，在图中找出从一个顶点出发，到另一个顶点的路径。   
## 1广度优先搜索（BFS）
直观地讲，它其实就是一种“地毯式”层层推进的搜索策略，即先查找离起始顶点最近的，然后是次近的，依次往外搜索。   
![avatar](pg/bfs.jpg)    
## 2深度优先搜索（DFS）
深度优先搜索（Depth-First-Search），简称 DFS。最直观的例子就是“走迷宫”。假设你站在迷宫的某个岔路口，然后想找到出口。你随意选择
一个岔路口来走，走着走着发现走不通的时候，你就回退到上一个岔路口，重新选择一条路继续走，直到最终找到出口。这种走法就是一种深度优先搜
索策略。   
# 25字符串匹配基础
## 1 BF算法
BF算法中的BF是Brute Force的缩写，中文叫做暴力匹配算法，也叫做朴素匹配算法。    
核心思想：我们在主串中，检查起始位置分别是0、1、2、3、、、n-m且长度为m的n-m+1个字串，看有没有跟模式串匹配的。
## 2RK算法
RK 算法的全称叫 Rabin-Karp 算法，是由它的两位发明者 Rabin 和 Karp 的名字来命名的。这个算法理解起来也不是很难。我个人觉得，它其
实就是刚刚讲的 BF 算法的升级版。   
RK 算法的思路是这样的：我们通过哈希算法对主串中的 n-m+1 个子串分别求哈希值，然后逐个与模式串的哈希值比较大小。如果某个子串的哈希值
与模式串相等，那就说明对应的子串和模式串匹配了（这里先不考虑哈希冲突的问题，后面我们会讲到）。因为哈希值是一个数字，数字之间比较是否
相等是非常快速的，所以模式串和子串比较的效率就提高了。   
## 3BM算法
BM算法的核心思想：在模式串与主串匹配的过程中，当模式串和主串某个字符不匹配的时候，能够跳过一些肯定不会匹配的情况，将模式串往后多滑动几位。   
**BM算法原理分析**   
BM算法包含两部分，分别是**坏字符规则**和**好后缀规则**。   
**如何选择坏字符规则和好后缀规则:**   
我们可以分别计算好后缀和坏字符往后滑动的位数，然后取两个数中最大的，作为模式串往后滑动的位数。这种处理方法可以避免坏字符规则计算出来的往后
滑动为负数的情况。   
# 26 Trie树
Trie树，也叫“字典树”。顾名思义，它是一种树形结构。它是一种专门处理字符串匹配的数据结构，用来解决在一组字符串集合中快速查找某个字符串的问题。   
**Trie 树的本质，就是利用字符串之间的公共前缀，将重复的前缀合并在一起。**   
例如how，hi，her，hello，so，see这几个字符串组成的Trie树结构如下：   
![avatar](pg/Trie.jpg)     
其中，根节点不包含任何信息。每个节点都表示一个字符串中的字符，从根节点到红色节点的一条路径表示一个字符串。   
# 26 AC自动机
AC 自动机算法，全称是 Aho-Corasick 算法。其实，Trie 树跟 AC 自动机之间的关系，就像单串匹配中朴素的串匹配算法，跟 KMP 算法之间的
关系一样，只不过前者针对的是多模式串而已。   
# 27 贪心算法
贪心算法解决问题的步骤：   
第一步，当我们看到这类问题的时候，首先要联想到贪心算法。针对一组数据，我们定义了限制值和期望值，希望从中选出几个数据，在满足限制的情况
下期望值最大。   
第二步，我们尝试看下这个问题是否可以用贪心算法解决。每次选择当前情况下，在对限制值同等贡献量的情况下，对期望值贡献最大的数据。   
第三步，我们举几个列子看下贪心算法产生的结果是不是最优的。   
# 28 分治算法
分治算法的核心就是四个字，分而治之，也就是将原问题划分成n个规模较小，并且结构与原问题相似的子问题，递归地解决这些子问题，然后再合并其结果
，就得到原问题的解。   
**分治算法是一种处理问题的思想，递归是一种编程技巧。**   
分治算法的递归实现中，每一层递归都会涉及这样三个操作：   
1. 分解：将原问题分解成一系列子问题。
2. 解决：递归地求解各个子问题，若子问题足够小，就直接求解。
3. 合并：将子问题的结果合并成原问题。
分治算法能解决的问题，一般需要满足下面这几个条件：   
1. 原问题与分解成的小问题具有相同的模式。
2. 原问题分解成的子问题可以独立求解，子问题之间没有相关性。
3. 具有分解终止条件，也就是说，当问题足够小时，可以直接求解。
4. 可以将子问题合并成原问题，而这个合并操作的复杂度不能太高，否则就起不到减小算法总体复杂度的效果了。   
# 29 回溯算法


